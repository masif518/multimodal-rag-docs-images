# Multimodal RAG: Question Answering over Documents & Images

This project implements a **Multimodal Retrieval-Augmented Generation (RAG)** system that answers user questions using information from **PDF documents and images**.

The system supports:
- Text extraction from PDFs
- OCR-based text extraction from images
- Semantic search using embeddings and FAISS
- Context-aware answer generation using a Large Language Model (LLM)

---

## ğŸš€ Features

- ğŸ“„ PDF text extraction using PyMuPDF  
- ğŸ–¼ï¸ Image text extraction using Tesseract OCR  
- ğŸ” Semantic retrieval using SentenceTransformers + FAISS  
- ğŸ§  LLM-based answering using a Retrieval-Augmented approach  
- â˜ï¸ Fully runnable on Google Colab  

---

## ğŸ—ï¸ Architecture

PDFs + Images
â†“
Text Extraction + OCR
â†“
Embeddings (SentenceTransformers)
â†“
FAISS Vector Store
â†“
Context Retrieval
â†“
LLM Prompting (RAG)
â†“
Answer


---

## ğŸ› ï¸ Tech Stack

- Python
- Google Colab
- PyMuPDF
- Tesseract OCR
- SentenceTransformers
- FAISS
- Hugging Face Transformers

---

## ğŸ“’ How to Run

1. Open the notebook in **Google Colab**
2. Upload PDFs and images when prompted
3. Run cells top to bottom
4. Ask questions over your documents and images

---

## ğŸ’¡ Example Use Cases

- Summarizing reports with charts
- Understanding scanned documents
- Question answering over mixed document types
- Multimodal knowledge retrieval

---

## ğŸ“Œ Notes

- Implemented manual RAG logic to avoid framework version drift
- Designed to be lightweight and CPU-friendly

---

## ğŸ‘¤ Author

**Masif Ahmed**
